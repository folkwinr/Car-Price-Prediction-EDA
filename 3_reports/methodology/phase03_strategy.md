# ğŸ§­ Phase 03 Methodology  
## Outlier Handling Strategy & Decision Logic

Outlier handling in Phase 03 was **not treated as a single â€œmagic formulaâ€**.  
Instead, decisions were made based on **column type** and **error nature**.

- ğŸ¯ If the **target (price)** is polluted, *all downstream analysis becomes misleading* â†’ price handled first.
- ğŸ§  When values were **clearly wrong or impossible**, we preferred **NaN + group-wise imputation** over aggressive row drops.
- ğŸ§± For **heavy-tailed distributions**, **IQR (Tukey Fence)** was favored over z-score.
- ğŸ›ï¸ For **discrete features** (e.g. `gears`, `previous_owner`), outlier logic was **domain-driven**, not statistical.
- âœ… After *every intervention*, we re-checked:
  - shape  
  - nulls  
  - unique values  
  - distributions  
  â†’ continuous *before vs. after* comparison.

---

## ğŸ§ª 1) SOP: Standard Flow Applied to Every Column

The same disciplined workflow was applied column by column.

---

### ğŸ” Profiling (Triage)

- Null ratio
- Number of unique values
- `value_counts`
- Extreme value lists (head / tail)

**Goal:**  
Is this column reliable?  
Is the issue a *true outlier* or a *data entry error*?

---

### ğŸ“Š Distribution Inspection (Evidence)

- Histogram
- Boxplot

Key questions:
- Is there a heavy right tail?
- Is the distribution bimodal?
- Does z-score make sense here, or is IQR safer?

---

### ğŸ§¾ Contextual Review of Extremes

- Sorted min/max values using `sort_values`
- Manual plausibility checks
- In some cases, inspected **make / model / body_type** context

---

### ğŸ§  Decision Rule Selection

- Impossible / invalid values â†’ **NaN + group-wise imputation**
- Extreme but plausible values â†’ **IQR-based drop** (especially price, mileage)
- Discrete nonsense (e.g. `gears == 0`, `owners >= 10`) â†’ **rule-based correction / NaN / drop**
- Final stabilization â†’ **z-style pruning** where appropriate

---

### âœ… Final Validation

- Are nulls back to zero?
- Did unique counts normalize?
- Does the distribution look stable?
- Was shape change logged?

---

## ğŸ§© 2) Decision Tree: What We Did â€” and Did Not Do

Think of this as an explicit **ifâ€“then logic**.

---

### âœ… A) Signal: â€œImpossible Valueâ€  
*(typo / unit error / parsing issue)*

**Example signals:**
- `engine_size = 99900`, `54009` â†’ unit confusion
- `empty_weight = 75 kg` â†’ physically impossible
- `co_emissions = 940` â†’ logically implausible

**â¡ï¸ What we did:**
- Converted values to **NaN**
- Imputed using **median or mode** within  
  `make_model Ã— body_type` groups

**â¡ï¸ What we did NOT do:**
- Did **not** apply z-score first  
  (std is already contaminated â†’ misleading pruning)
- Did **not** use global median  
  (vehicle groups are heterogeneous â†’ meaningless fill)

---

### âœ… B) Signal: â€œHeavy Tail + Naturally Extreme Featureâ€

**Example signals:**
- `price` and `mileage` show classic heavy-tail behavior
- Boxplots dominated by long right tails

**â¡ï¸ What we did:**
- Applied **IQR (Tukey Fence 1.5Ã—IQR)** â†’ drop
- For `price`, additionally applied **model-based anomaly rules first**

**â¡ï¸ What we did NOT do:**
- Did **not** default to z-score  
  (on skewed distributions, z-score can be overly aggressive or incorrect)

---

### âœ… C) Signal: â€œDiscrete Column + Illogical Valuesâ€

**Example signals:**
- `gears = 0` â†’ often means â€œunknownâ€
- `gears > 8` â†’ suspicious in this dataset context
- `previous_owner >= 10` â†’ extremely rare + quality concern

**â¡ï¸ What we did:**
- Applied **explicit domain rules**: NaN / correction / drop
- Used **mode imputation** when appropriate  
  (mode is more meaningful for discrete variables)

**â¡ï¸ What we did NOT do:**
- Did **not** treat these as statistical outliers  
  (boxplot / z-score are misleading for discrete features)

---

## âœ… 3) Quality Gates Applied at Every Step

After each cleaning action, we verified four checkpoints:

- ğŸ“ **Shape change**  
  (How many rows / columns were affected?)
- ğŸ•³ï¸ **Null check**  
  (Did imputation resolve NaNs correctly?)
- ğŸ§® **Unique count sanity**  
  (Did discrete features normalize?)
- ğŸ“‰ **Distribution stability**  
  (Were tails reduced? Boxplots interpretable?)

ğŸ‘‰ This is why Phase 03 is **not a one-shot cleanup**,  
but an **iterative decision + validation loop**.

---

## ğŸ§  4) What We Explicitly Did NOT Do (By Design)

- ğŸš« No fully automatic global outlier detectors  
  (Isolation Forest, LOF, etc.)  
  â†’ Phase 03 focused on **EDA + explainable cleaning**

- ğŸš« No multivariate outlier analysis  
  â†’ Deferred to **Phase 05** via model residual diagnostics

- ğŸš« No permanent winsorization or log transforms everywhere  
  â†’ Used only for **visual comparison**, not as default data transformation

---

## ğŸ“Œ Outcome: What This Methodology Achieved

- ğŸ“‰ Reduced dominance of extreme values in modeling
- ğŸ§± Stabilized numeric feature distributions
- ğŸ§¹ Removed noise-heavy columns and duplicates
- âœ… Delivered a **clean, explainable, model-ready dataset**

