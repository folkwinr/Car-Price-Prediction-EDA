# ğŸ§© Part-02 Methodology (Handling Missing Values) â€” What We Thought, What We Did (Step-by-Step)

> ğŸ¯ **Goal of Part-02:**  
> Take `clean_scout2022.csv` (already cleaned in Part-01) and produce a dataset where **missing values are handled in a controlled, explainable way**, using:
> - **group-based imputation (grouping + fallback)**
> - **domain rules** (especially Electric cars)
> - **light feature engineering** (turn long equipment text into packages)
>
> âœ… **Input:** `clean_scout2022.csv`  
> âœ… **Output:** `filled_scout2022.csv`

---

## 0) ğŸ§  Core Idea (The â€œwhyâ€ behind everything)
Scraped car listing data is messy, but it is *structured messy*:
- Many values are missing because sellers donâ€™t fill them.
- A lot of variables are *stable* inside a segment:
  - within the same **make_model**, **body_type**, **gearbox**, **fuel_type**, **age**
- So instead of random filling, we used **context-aware filling**:
  - â€œIf I know the model and body type, I can guess doors / engine size / drivetrain much better.â€

âœ… **Main principle:**  
**Fix the â€œgroup keysâ€ first**, then fill the rest using those keys.

---

## 1) ğŸ§° Setup + Quick Health Check (Before touching missing values)

### 1.1 ğŸ“¥ Load and protect data
- We loaded the cleaned dataset and created a copy so we donâ€™t damage the source.

```python
df0 = pd.read_csv("clean_scout2022.csv")
df  = df0.copy()
```

### 1.2 ğŸ” Quick baseline checks
We used:
- `df.shape`, `df.info()` to understand types and size
- repeated `first_looking(col)` to see:
  - % missing
  - unique values
  - value_counts (including NaN)

âœ… Why this matters:
- We didnâ€™t want to â€œfill blindlyâ€.  
- We first **measured** each columnâ€™s missing and patterns.

---

## 2) ğŸ§± Step-By-Step Pipeline (Our actual working order)

### âœ… Step 1 â€” Make grouping keys stable (because everything depends on them)
**Why first?**  
If `model` is missing, then `(make_model + body_type)` groups become weak and imputation becomes wrong.

#### 1A) Repair `model` using `short_description` (regex extraction)
What we did:
- We created a big `model_pattern`
- We extracted models from `short_description`
- We filled missing `model` with extracted results
- If still missing â†’ we inspected remaining rows and **dropped unrecoverable ones**

```python
df["extracted_models"] = (
    df["short_description"].str.extract(model_pattern, flags=re.IGNORECASE)
).bfill(axis=1)[0]

df["model"] = df["model"].fillna(df["extracted_models"])
df.dropna(subset=["model"], inplace=True)  # remove the last unrecoverable records
```

ğŸ“Œ What we observed (from notebook notes):
- `model` had **276 NaNs before**
- after regex fill, **6 rows** still had NaN â†’ we **dropped those 6 rows**

#### 1B) Rebuild `make_model` to be consistent
We assumed `make + model` is a more reliable identity than raw `make_model` when model is fixed.

```python
df["modified_make_model"] = df["make"] + " " + df["model"]
# use it to update make_model
```

Then we standardized formatting:
- `.str.title()` on `make_model`, `make`, `model`, and also `body_type`  
(to remove writing differences like `FORD mustang` vs `Ford Mustang`)

âœ… Finally, we removed helper columns:
```python
df.drop(["modified_make_model", "short_description", "extracted_models"], axis=1, inplace=True)
```

---

### âœ… Step 2 â€” Use a â€œMissing Pattern Lensâ€ before filling (we donâ€™t fill immediately)
For many columns we used the same technique:

#### ğŸ‘ï¸ â€œDash trickâ€ (temporary visibility trick)
We temporarily replaced NaN with `"-"` to:
- make missing values visible in `describe()` and `value_counts`
- filter rows with missing quickly (`df[col] == "-"`)

Then we switched back to real missing (`np.nan`) before actual filling.

```python
df[col].fillna("-", inplace=True)   # inspect patterns
# ... explore patterns (groupby, value_counts, describe)
df[col].replace("-", np.nan, inplace=True)  # ready for real fill
```

âœ… Why we did this:
- NaN is â€œinvisibleâ€ in many text checks.
- Using `"-"` helped us *see* hidden patterns and take correct decisions.

---

### âœ… Step 3 â€” Choose fill method based on column nature (Decision Rules)
We did **not** use a single method for all columns.

#### 3.1 ğŸ§© If column is categorical (categories/labels) â†’ MODE
Examples:
- `doors`, `gearbox`, `fuel_type`, `drivetrain`, `seats`, `engine_size` *(treated as stable segment value)*

**Reason:**  
Inside `(make_model + body_type)` the most common value is often the correct one.

We used a two-level fallback logic:
1) group by `(group_col1 + group_col2)`
2) fallback to group by `(group_col1)`
3) fallback to global mode

âœ… Implemented in our main engine function `fill(..., method="mode")`

```python
fill(df, "make_model", "body_type", "doors", "mode")
fill(df, "make_model", "body_type", "gearbox", "mode")
fill(df, "make_model", "gearbox",   "fuel_type", "mode")
```

#### 3.2 ğŸ”¢ If column is numeric but can have real outliers â†’ MEDIAN
We preferred median for numeric columns that can be skewed:
- `co_emissions`
- `cons_avg`

**Reason:**  
Mean is sensitive to extreme values; median is more robust.

We used **multi-level median fallback** with `fill_median()`:
1) `(make_model + body_type + fuel_type)`
2) `(make_model + body_type)`
3) `(make_model)`
4) global median

```python
fill_median(df, "make_model", "body_type", "fuel_type", "co_emissions")
fill_median(df, "make_model", "body_type", "fuel_type", "cons_avg")
```

#### 3.3 ğŸ“ˆ If column is numeric and strongly tied to a segment average â†’ MEAN
We used mean where it makes sense:
- `mileage` (depending on `make_model` and `age`)

**Reason:**  
Mileage behaves like an average pattern within (model, age) groups.

```python
df["mileage"].fillna(
    df.groupby(["make_model", "age"]).mileage.transform("mean"),
    inplace=True
)
```

#### 3.4 ğŸ” If column behaves like a â€œpropagated attributeâ€ inside a group â†’ FFILL + BFILL
We used forward/back fill where the value is not â€œcomputedâ€, but often repeated in a structured way inside groups:
- `upholstery` (after category redesign)
- `previous_owner` (within `age` groups)

**Reason:**  
When groups are consistent, ffill/bfill fills gaps smoothly without forcing a global category.

```python
fill(df, "make_model", "body_type", "upholstery", "ffill")
fill_prop(df, "age", "previous_owner")
```

âš ï¸ Important note:
- We used ffill/bfill **inside groups** first (safer than global ffill directly).

---

## 4) ğŸ§  Special Rules (Domain Knowledge Overrides)
Some columns cannot be filled correctly without â€œcar logicâ€.

### 4.1 ğŸ§¾ Warranty â†’ convert to Yes/No using a rule
We did not try to guess warranty months/years.  
We simplified it into a strong signal.

```python
df["warranty"].fillna("-", inplace=True)

def warrantyclassifier(x):
    return "No" if "-" in x else "Yes"

df["warranty"] = df["warranty"].astype(str).apply(warrantyclassifier)
```

âœ… Why this is good:
- Warranty presence is often more useful than exact value.
- Less noise, more model-friendly.

### 4.2 âš¡ Electric cars & `co_emissions`
We checked Electric emissions separately:
- Electric cars should mostly be near **0** emissions.
- So we filled missing emissions for Electric cars using **Electric mode** first.

```python
df["co_emissions"].fillna("-", inplace=True)
df.loc[df["fuel_type"] == "Electric", "co_emissions"] = (
    df.loc[df["fuel_type"] == "Electric", "co_emissions"]
      .replace("-", np.nan)
      .fillna(df.loc[df["fuel_type"] == "Electric", "co_emissions"].mode()[0])
)
```

Then we applied the global 3-level median strategy for all cars.

âœ… Why this order:
- Electric cars are a special distribution.  
- If we mix them with fuel cars too early, the fill can become wrong.

### 4.3 âš¡ Electric cars & `cons_avg`
We used a domain constant for Electric consumption:

```python
df.loc[df["fuel_type"] == "Electric", "cons_avg"] = 2.359
```

âœ… Why:
- Electric consumption behaves very differently.
- This prevents â€œfuel-car-likeâ€ imputation from corrupting Electric rows.

### 4.4 ğŸª‘ Upholstery redesign before filling
We reduced category chaos using domain knowledge:
- â€œVelourâ€ â†’ â€œClothâ€
- â€œalcantara / Part leather / Full leatherâ€ â†’ â€œPart/Full Leatherâ€
- â€œOtherâ€ â†’ NaN (noise category)

Then we filled with group ffill/bfill.

```python
df["upholstery"].replace(
    ["Velour", "alcantara", "Part leather", "Full leather"],
    ["Cloth",  "Part/Full Leather", "Part/Full Leather", "Part/Full Leather"],
    inplace=True
)
df["upholstery"].replace("Other", np.nan, inplace=True)
fill(df, "make_model", "body_type", "upholstery", "ffill")
```

âœ… Why this order:
- If we fill first, we propagate messy labels.
- If we standardize first, we propagate clean categories.

### 4.5 ğŸ§“ Age rule using mileage (small but smart)
We inspected missing ages and mileage patterns.
If mileage is very low, a missing age likely means **new car**.

```python
df["age"].fillna("-", inplace=True)
cond_age1 = (df["mileage"] < 10000)
df.loc[cond_age1, "age"] = df.loc[cond_age1, "age"].replace("-", 0)
```

âœ… Why:
- A very low mileage vehicle often means age â‰ˆ 0 in listing contexts.
- This made age logic more consistent before other grouped fills.

---

## 5) ğŸ§ª Engineering Features during Part-02 (Not only â€œfillâ€, also â€œimproveâ€)
Some raw columns were long text (equipment lists). Instead of keeping them as-is:
- we filled them (so we have stable content)
- then we built **package-level features**
- then we dropped raw long-text columns (less noise)

### 5.1 ğŸ§© Comfort package
We created:
- `comfort_convenience_Package` with rule-based keywords:
  - Premium Plus / Premium / Standard

```python
premium = [...]
premium_plus = [...]

df["comfort_convenience_Package"] = df["comfort_convenience"].apply(
    lambda s: "Premium Plus" if all(w in s for w in premium_plus)
    else ("Premium" if all(w in s for w in premium) else "Standard")
)

df.drop("comfort_convenience", axis=1, inplace=True)
```

### 5.2 ğŸµ Entertainment package
Created:
- `entertainment_media_Package` (Media Plus vs Standard Media)

```python
media_plus = [...]
df["entertainment_media_Package"] = df["entertainment_media"].apply(
    lambda s: "Media Plus" if any(w in s for w in media_plus) else "Standard Media"
)
df.drop("entertainment_media", axis=1, inplace=True)
```

### 5.3 ğŸ›¡ï¸ Safety package
Created:
- `safety_security_Package` (Premium / Premium Plus / Standard)

```python
premium = [...]
premium_plus = [...]
df["safety_security_Package"] = df["safety_security"].apply(...)
df.drop("safety_security", axis=1, inplace=True)
```

âœ… Why we engineered packages:
- Long equipment text is hard to model directly.
- Package level gives a strong, compact signal.

---

## 6) ğŸ—‘ï¸ â€œDrop Strategyâ€ in Part-02 (What we removed and why)
We dropped columns for clear reasons:

### 6.1 Helper columns (only for fixing other columns)
- `modified_make_model`, `short_description`, `extracted_models`

âœ… Reason: they were only tools; keeping them adds noise.

### 6.2 Redundant columns (same signal already exists)
- `power_hp` (redundant with `power_kW`)
- `cons_city`, `cons_country` (redundant after choosing `cons_avg` as main)
- `cylinders` (considered low value / redundant with engine_size + power)

âœ… Reason: keep the dataset focused and avoid multicollinearity/noise.

---

## 7) âœ… Quality Gates (How we verified our fills)
We repeatedly used:
- `first_looking(col)` before and after filling
- group-based summaries like `groupby(...).describe()` to detect patterns and sanity-check
- every fill function printed:
  - % null after fill
  - number of nulls
  - unique count
  - value counts

âœ… This ensured:
- we didnâ€™t â€œfill with nonsenseâ€
- we didnâ€™t break category distributions silently
- we could spot anomalies (example: Electric with non-zero emissions)

---

## 8) ğŸ’¾ Final Deliverable
At the end:
- dataset had far fewer missing values
- grouping keys were stable
- engineered package columns existed
- helper/redundant columns were removed

```python
df.to_csv("filled_scout2022.csv", index=False)
```

---

# ğŸ¯ Mini Decision Tree (Very Clear)
- **If `model` missing** â†’ extract from `short_description` (regex) â†’ if still missing â†’ drop row  
- **If categorical** (doors/gearbox/fuel_type/drivetrain/seats) â†’ `mode` with fallback groups  
- **If numeric & skewed** (co_emissions/cons_avg) â†’ `median` with 3-level fallback  
- **If numeric & segment-average** (mileage) â†’ `mean` by `(make_model, age)`  
- **If propagated attribute** (previous_owner/upholstery) â†’ group `ffill+bfill`  
- **If domain-specific** (Electric) â†’ handle Electric first (special rule), then general fill  
- **If long text equipment** â†’ fill â†’ create `*_Package` â†’ drop raw text column  
- **If redundant** â†’ drop
