# ğŸš— Car Price Prediction EDA (AutoScout24) â€” 3-Phase Data Preparation Pipeline

A complete, step-by-step data preparation project for car listings data, built as an EDA capstone.

âœ… **Phase 1:** Data Cleaning  
âœ… **Phase 2:** Handling Missing Values  
âœ… **Phase 3:** Handling Outliers + Final Sanity Checks (**END OF PROJECT**)

> âš ï¸ **Important Scope Note**  
> The project **ends after Phase 3 â€œFinal Stepâ€** (outlier handling + correlation check + duplicate removal).  
> We **do not include**: Business Questions, Inferential Statistics, or Dummy/Encoding steps.

---

## ğŸ“Œ Table of Contents
- [ğŸ¯ Project Goal](#-project-goal)
- [ğŸ§¾ Dataset](#-dataset)
- [ğŸ—‚ Repository Structure](#-repository-structure)
- [ğŸ”„ Workflow Overview](#-workflow-overview)
  - [Phase 1 â€” ğŸ§¼ Data Cleaning](#phase-1--data-cleaning)
  - [Phase 2 â€” ğŸ§© Handling Missing Values](#phase-2--handling-missing-values)
  - [Phase 3 â€” ğŸš¨ Handling Outliers + Final Step](#phase-3--handling-outliers--final-step)
- [ğŸ“¦ Outputs](#-outputs)
- [â–¶ï¸ How to Run](#ï¸-how-to-run)
- [ğŸ§  Key Design Decisions](#-key-design-decisions)
- [âš ï¸ Limitations](#ï¸-limitations)
- [ğŸš€ Next Steps (Optional)](#-next-steps-optional)

---

## ğŸ¯ Project Goal
Prepare messy, real-world car listing data for analysis and modeling by:
- cleaning formats and data types,
- handling missing values with smart, explainable rules,
- detecting and handling outliers using domain knowledge + robust statistics,
- removing duplicates and checking strong correlations in the final dataset.

---

## ğŸ§¾ Dataset
**Source format:** JSON (`as24_cars.json`)  
**Domain:** second-hand car listings (AutoScout24-style structure)

Typical feature groups:
- **Identity:** make/model/make_model  
- **Listing:** price, seller, location  
- **Technical:** engine_size, power_kW, gearbox, gears, drivetrain, empty_weight  
- **Usage:** mileage, age, previous_owner  
- **Eco:** fuel_type, co_emissions, cons_avg  
- **History/Other:** full_service_history, warranty, extras, upholstery, energy class

---

## ğŸ—‚ Repository Structure
Recommended structure (you can adjust names as you like):

```text
â”œâ”€ notebooks/
â”‚  â”œâ”€ EDA_scout_car_phase_1 (Data_Cleaning)_ONDIA.ipynb
â”‚  â”œâ”€ EDA_scout_car_phase_2 (Handling_Missing_Values)_ONDIA.ipynb
â”‚  â””â”€ EDA_scout_car_phase_3 (Handling_Outliers)_ONDIA_V2.ipynb
â”‚
â”œâ”€ data/
â”‚  â””â”€ as24_cars.json
â”‚
â”œâ”€ outputs/
â”‚  â”œâ”€ clean_scout2022.csv
â”‚  â””â”€ filled_scout2022.csv
â”‚
â””â”€ README.md
```

---

## ğŸ”„ Workflow Overview

### Phase 1 â€” ğŸ§¼ Data Cleaning
**Notebook:** `EDA_scout_car_phase_1 (Data_Cleaning)_ONDIA.ipynb`  
**Input:** `as24_cars.json`  
**Output:** `clean_scout2022.csv`

What we did:
- ğŸ”¤ **Standardized column names** (snake_case) and checked mixed types.
- ğŸ§¹ **Cleaned key fields** (format fixes, consistent text formatting, safe conversions).
- ğŸ”¢ Converted numeric-like text into real numeric columns (price, mileage, engine_size, etc.).
- ğŸ§  Built early feature engineering where needed (example: split power and consumption fields).
- ğŸ—‘ Dropped columns that were:
  - extremely missing,
  - irrelevant for analysis,
  - ID/leakage-like (example: offer number),
  - free-text not used in this phase.
- âœ… Produced a clean baseline dataset ready for a strong missing-value strategy.

ğŸ“¦ **Export:** `clean_scout2022.csv`

---

### Phase 2 â€” ğŸ§© Handling Missing Values
**Notebook:** `EDA_scout_car_phase_2 (Handling_Missing_Values)_ONDIA.ipynb`  
**Input:** `clean_scout2022.csv`  
**Output:** `filled_scout2022.csv`

Main idea:
> Missing values are not filled randomly. We fill them using **car segment context**.

What we did (high-impact steps):
- ğŸ§  **Fixed group keys first** (because the whole filling logic depends on them):
  - repaired missing `model` using **regex extraction** from `short_description`,
  - rebuilt `make_model = make + model`,
  - standardized formatting (Title Case) for consistent grouping.
- ğŸ§® **Used different fill methods based on column type:**
  - **Mode** for categorical columns (doors, gearbox, fuel_type, drivetrain, seats, etc.),
  - **Median** for skewed numeric columns (co_emissions, cons_avg),
  - **Mean** for mileage inside strong segments (`make_model + age`),
  - **ffill/bfill** inside groups for propagation-like fields (previous_owner, upholstery).
- âš¡ **Applied Electric-car domain rules**
  - handled special behavior for `co_emissions` and `cons_avg` to avoid wrong values.
- ğŸ§ª **Feature engineering during filling**
  - built package-level features from long equipment text:
    - `comfort_convenience_Package`
    - `entertainment_media_Package`
    - `safety_security_Package`
  - dropped raw long-text columns after creating better features (less noise).
- ğŸ—‘ Dropped redundant or low-value columns to keep the dataset focused:
  - redundant: `power_hp` (kept `power_kW`),
  - redundant: `cons_city`, `cons_country` (kept `cons_avg`),
  - low-value: `cylinders`,
  - helper columns used only for repair.

ğŸ“¦ **Export:** `filled_scout2022.csv`

---

### Phase 3 â€” ğŸš¨ Handling Outliers + Final Step
**Notebook:** `EDA_scout_car_phase_3 (Handling_Outliers)_ONDIA_V2.ipynb`  
**Input:** `filled_scout2022.csv`  
**Output:** Final DataFrame after outliers + final checks (**END OF PROJECT**)

Core philosophy:
> Outliers are not all the same. Some are real, some are data errors, and some harm model quality.  
> We used a mix of **domain rules** and **robust statistics** to handle them.

#### 1) Outlier detection tools we used
- ğŸ§  **Domain rules** (impossible/unrealistic values)
- ğŸ“ **Tukeyâ€™s Fence (IQR)** for heavy-tailed variables
- ğŸ§® **z-score style filtering** for extreme numeric cases

#### 2) Column-level outlier actions (what we did)
âœ… **price**
- Removed clear anomaly cases by domain filters (specific `make_model` + extreme price levels).
- Applied **Tukey Fence** (IQR) and removed remaining extreme rows.

âœ… **mileage**
- Dropped mileage values above **1,000,000** (domain-based).
- Applied **Tukey Fence** to remove extreme mileage rows.

âœ… **engine_size**
- Marked known invalid values as **NaN** (very small or extremely large values).
- Filled missing using segment **mode** (`make_model + body_type`).
- Applied z-score filtering to remove remaining extreme rows.

âœ… **gears**
- Set `gears == 0` or `gears > 8` to **NaN**, then filled with segment **mode**.
- Dropped `gears == 2` cases (treated as suspicious in this dataset).

âœ… **empty_weight**
- Set `empty_weight > 4000` to **NaN**.
- Replaced known invalid values (example: 75, 525) with **NaN**.
- Filled missing using segment **mode**.

âœ… **co_emissions**
- Marked specific extreme emissions values as **NaN** (example: 940, 910, 420, 414).
- Filled via segment **median**.
- Used `log1p` inspection to review distribution behavior.
- Applied z-score filtering for the most extreme rows.

âœ… **cons_avg**
- Set values `>= 20` to **NaN** (domain-based).
- Filled via segment **median**.
- Applied z-score filtering for extreme rows.

âœ… **previous_owner**
- Dropped rows where `previous_owner >= 10` (treated as extreme/unreliable).

âœ… **age**
- Dropped rows where `age > 20` or `age < 0`.

ğŸ—‘ Dropped columns during Phase 3:
- `doors`
- `seats`

#### 3) Final Step (end of project)
After outlier handling:
- ğŸ”¥ Checked correlation structure to understand strong relationships.
- ğŸ§¾ Checked duplicates and removed them:
  - `df.drop_duplicates(inplace=True)`

âœ… This is the final point of the project pipeline.

> ğŸ’¾ Optional: if you want to save the final dataset, add this line at the end of Phase 3:
```python
df.to_csv("scout_outliers_handled_final.csv", index=False)
```

---

## ğŸ“¦ Outputs
- âœ… `clean_scout2022.csv` â†’ output of Phase 1  
- âœ… `filled_scout2022.csv` â†’ output of Phase 2  
- âœ… Final DataFrame after Phase 3 â†’ outliers handled + duplicates removed  
  - (optional export: `scout_outliers_handled_final.csv`)

---

## â–¶ï¸ How to Run

### 1) Create environment
```bash
python -m venv .venv
source .venv/bin/activate   # macOS/Linux
# .venv\Scripts\activate    # Windows
```

### 2) Install packages
```bash
pip install -U pip
pip install pandas numpy matplotlib seaborn scipy scikit-learn ipywidgets termcolor
```

### 3) Add the raw data
Place the raw JSON file here:
```text
data/as24_cars.json
```

### 4) Run notebooks in order
1) Phase 1 â†’ exports `clean_scout2022.csv`  
2) Phase 2 â†’ exports `filled_scout2022.csv`  
3) Phase 3 â†’ outlier handling + correlation check + duplicate removal (**end**)

---

## ğŸ§  Key Design Decisions
- âœ… **Fix group keys first** (`model`, `make_model`, `body_type`) â†’ stronger filling + cleaner segments.
- âœ… **Use the right imputation method for the job:**
  - mode for categorical,
  - median for skewed numeric,
  - mean for stable segment averages,
  - ffill/bfill only inside meaningful groups.
- âœ… **Outlier handling is two-layered:**
  1) domain knowledge rules,
  2) robust statistical rules (IQR / z-score).
- âœ… Prefer **â€œNaN then fillâ€** when the value is likely a data error but the row is still valuable.
- âœ… Drop rows only when values are clearly unreliable and harm dataset quality.
- âœ… Remove duplicates at the end to prevent repeated records from influencing analysis/modeling.

---

## âš ï¸ Limitations
- Thresholds are dataset-specific and based on the segment focus in this project.
- Some extreme cars may be real (luxury/collector vehicles) but can still be removed by IQR/z-score.
- The final CSV export is optional (add `to_csv(...)` in Phase 3 if needed).

---

## ğŸš€ Next Steps (Optional)
If you continue after this pipeline, typical next steps are:
- encoding categorical features (one-hot / label encoding),
- scaling numeric features,
- training baseline models and evaluating performance.

(These are intentionally out of scope for the current project version.)


